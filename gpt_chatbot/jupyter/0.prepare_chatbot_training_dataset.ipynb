{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4303c52d-0522-49ad-aa6b-796f189997b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step-1 Get Access to Raw Error Log Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf3576e2-d905-451c-b681-78fe05692890",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark get access to filesystem's authorization \n",
    "access_key = \"\"\n",
    "spark.conf.set(\"fs.azure.account.key.emmadatalakesa.dfs.core.windows.net\", access_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebb52428-6420-4022-b404-3f2df9331e52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>abfss://idm@emmadatalakesa.dfs.core.windows.net/error_log/error_log</td><td>error_log</td><td>3092</td><td>1688130446000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "abfss://idm@emmadatalakesa.dfs.core.windows.net/error_log/error_log",
         "error_log",
         3092,
         1688130446000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load error log raw data from specified error log path with schma app:string, timestamp: YYYY-MM-dd, log_content: string\n",
    "# our training is daily model training, so, timestamp is YYYY-MM-dd  \n",
    "error_log_path = \"abfss://idm@emmadatalakesa.dfs.core.windows.net/error_log/\"\n",
    "display(dbutils.fs.ls(error_log_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "456e34fb-7cb6-4fb1-94fa-6bc2766ac2a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step-2 Load Raw Error Log Dataset from FileSystem to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2379c68e-6810-4085-bf47-94b02fe7e58f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "# app: string, \n",
    "# timestamp: string,\n",
    "# log_content: string\n",
    "error_log_schema = StructType(fields = [\n",
    "    StructField(\"app\", StringType(), False),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"log_content\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f3b07dc-f097-47e6-a8fd-37937e8f9edd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- app: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- log_content: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>app</th><th>timestamp</th><th>log_content</th></tr></thead><tbody><tr><td>idm</td><td>2023/6/30</td><td>Invalid user name/password or user account is locked or expired</td></tr><tr><td>idm</td><td>2023/6/30</td><td>Invalid credential with user name and password</td></tr><tr><td>idm</td><td>2023/6/30</td><td>DB CONNECTION URL IS :jdbc:postgresql://usg4-smax-rds-prod.clkmmx1qw2cg.us-gov-west-1.rds.amazonaws.com:5432/itom-cdf-idm?ssl=true&sslmode=verify-full&sslfactory=com.hp.ccue.identity.installer.factory.PostgresFactory&sslrootcert=/tmp/dbCerts.crt\\nUsing user: cdfidm\\nExceptions thrown: SSL error: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\\nException found when creating the connection and try again after 5 seconds\\nChecking time left -2000 milli seconds\\n\\nFail to execute command! [ sh idm.sh -h ] can show the usage of the CLI\\nStarting db build @ Mon May 23 23:52:00 UTC 2022\\n2022-05-23T23:52:00.909160500+00:00 INFO DATABASE_CONNECTION_URL is not defined.\\n2022-05-23T23:52:00.926541625+00:00 INFO build DB with ADMIN_DATABASE_CONNECTION_URL as jdbc:postgresql://usg4-smax-rds-prod.clkmmx1qw2cg.us-gov-west-1.rds.amazonaws.com:5432/postgres?ssl=true&sslmode=verify-full&sslfactory=com.hp.ccue.identity.database.postgresql.PostgresFactory&sslrootcert=/tmp/dbCerts.crt\\n2022-05-23T23:52:00.939900353+00:00 INFO Start to build postgres schema...\\nBuild postgres...\\nSSL error: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\\n\\nFail to execute command! [ sh idm.sh -h ] can show the usage of the CLI\\n</td></tr><tr><td>idm</td><td>2023/6/30</td><td>IdM pods don't restart automatically after a broken database recovers</td></tr><tr><td>idm</td><td>2023/6/30</td><td>sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target</td></tr><tr><td>idm</td><td>2023/6/30</td><td>ERROR: value too long for type character varying(255)</td></tr><tr><td>idm</td><td>2023/6/30</td><td>The request token is invalid</td></tr><tr><td>idm</td><td>2023/6/30</td><td>SMAL errors during login The request token is invalid </td></tr><tr><td>idm</td><td>2023/6/30</td><td>The request token is invalid. It may have already been used or expired. (Code: 40002)</td></tr><tr><td>idm</td><td>2023/6/30</td><td>ERROR [https-abcd-efg-8443-exec-41] com.company.ccue.identity.saml2.authentication.IdmSamlAuthenticationProvider [] - Validate assertion - An error occurred while validate the assertion: Authentication statement is too old to be used with value</td></tr><tr><td>idm</td><td>2023/6/30</td><td>com.company.ccue.identity.saml2.authentication.IdmSamlAuthenticationProvider</td></tr><tr><td>idm</td><td>2023/6/30</td><td>error: unable to upgrade connection: Not Found</td></tr><tr><td>idm</td><td>2023/6/30</td><td>Unable to upgrade connection</td></tr><tr><td>idm</td><td>2023/6/30</td><td>The service is unavailable at this time</td></tr><tr><td>idm</td><td>2023/6/30</td><td>can\\'t log in</td></tr><tr><td>idm</td><td>2023/6/30</td><td>The service is unavailable at this time. Please try again later.</td></tr><tr><td>idm</td><td>2023/6/30</td><td>IdmSamlAuthenticationProvider Authentication statement is too old to be used with value</td></tr><tr><td>idm</td><td>2023/6/30</td><td>ERROR [https-abcd-efg-8443-exec-41] com.company.ccue.identity.saml2.authentication.IdmSamlAuthenticationProvider [] - Validate assertion - An error occurred while validate the assertion: Authentication statement is too old to be used with value</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "idm",
         "2023/6/30",
         "Invalid user name/password or user account is locked or expired"
        ],
        [
         "idm",
         "2023/6/30",
         "Invalid credential with user name and password"
        ],
        [
         "idm",
         "2023/6/30",
         "DB CONNECTION URL IS :jdbc:postgresql://usg4-smax-rds-prod.clkmmx1qw2cg.us-gov-west-1.rds.amazonaws.com:5432/itom-cdf-idm?ssl=true&sslmode=verify-full&sslfactory=com.hp.ccue.identity.installer.factory.PostgresFactory&sslrootcert=/tmp/dbCerts.crt\\nUsing user: cdfidm\\nExceptions thrown: SSL error: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\\nException found when creating the connection and try again after 5 seconds\\nChecking time left -2000 milli seconds\\n\\nFail to execute command! [ sh idm.sh -h ] can show the usage of the CLI\\nStarting db build @ Mon May 23 23:52:00 UTC 2022\\n2022-05-23T23:52:00.909160500+00:00 INFO DATABASE_CONNECTION_URL is not defined.\\n2022-05-23T23:52:00.926541625+00:00 INFO build DB with ADMIN_DATABASE_CONNECTION_URL as jdbc:postgresql://usg4-smax-rds-prod.clkmmx1qw2cg.us-gov-west-1.rds.amazonaws.com:5432/postgres?ssl=true&sslmode=verify-full&sslfactory=com.hp.ccue.identity.database.postgresql.PostgresFactory&sslrootcert=/tmp/dbCerts.crt\\n2022-05-23T23:52:00.939900353+00:00 INFO Start to build postgres schema...\\nBuild postgres...\\nSSL error: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\\n\\nFail to execute command! [ sh idm.sh -h ] can show the usage of the CLI\\n"
        ],
        [
         "idm",
         "2023/6/30",
         "IdM pods don't restart automatically after a broken database recovers"
        ],
        [
         "idm",
         "2023/6/30",
         "sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target"
        ],
        [
         "idm",
         "2023/6/30",
         "ERROR: value too long for type character varying(255)"
        ],
        [
         "idm",
         "2023/6/30",
         "The request token is invalid"
        ],
        [
         "idm",
         "2023/6/30",
         "SMAL errors during login The request token is invalid "
        ],
        [
         "idm",
         "2023/6/30",
         "The request token is invalid. It may have already been used or expired. (Code: 40002)"
        ],
        [
         "idm",
         "2023/6/30",
         "ERROR [https-abcd-efg-8443-exec-41] com.company.ccue.identity.saml2.authentication.IdmSamlAuthenticationProvider [] - Validate assertion - An error occurred while validate the assertion: Authentication statement is too old to be used with value"
        ],
        [
         "idm",
         "2023/6/30",
         "com.company.ccue.identity.saml2.authentication.IdmSamlAuthenticationProvider"
        ],
        [
         "idm",
         "2023/6/30",
         "error: unable to upgrade connection: Not Found"
        ],
        [
         "idm",
         "2023/6/30",
         "Unable to upgrade connection"
        ],
        [
         "idm",
         "2023/6/30",
         "The service is unavailable at this time"
        ],
        [
         "idm",
         "2023/6/30",
         "can\\'t log in"
        ],
        [
         "idm",
         "2023/6/30",
         "The service is unavailable at this time. Please try again later."
        ],
        [
         "idm",
         "2023/6/30",
         "IdmSamlAuthenticationProvider Authentication statement is too old to be used with value"
        ],
        [
         "idm",
         "2023/6/30",
         "ERROR [https-abcd-efg-8443-exec-41] com.company.ccue.identity.saml2.authentication.IdmSamlAuthenticationProvider [] - Validate assertion - An error occurred while validate the assertion: Authentication statement is too old to be used with value"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "app",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "log_content",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_log_df = spark.read.option(\"header\", True)\\\n",
    "    .schema(error_log_schema) \\\n",
    "    .csv(error_log_path) \n",
    "# after load dataset by specific schema, print schema and print data from dataframe \n",
    "error_log_df.printSchema()\n",
    "error_log_df = error_log_df.filter(error_log_df.app == 'idm')\n",
    "display(error_log_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0943e057-1932-40af-9364-b51c2c92ef3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step -3 Load Json(embedded) Datasets from Local file to Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "141fcf25-8fae-446d-936c-bd4096194964",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### step 3-1 Install Python 3rd Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04ef4555-2960-4e58-8010-4bc070e6c11a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\n",
      "Python interpreter will be restarted.\n",
      "Python interpreter will be restarted.\n",
      "Python interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install redis > nohup\n",
    "%pip install jsonpickle > nohup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "023501a1-9901-41fc-b93a-45cc45d10828",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### step 3-2 Execute Python Scripts to Load Error Diagnoise Related Datasets to Redis Hash Set \n",
    "\n",
    "##### solutions_dataset.json \n",
    "* Dataset Description: solutions_dataset.json stores different errors' solutoin steps and causes\n",
    "* Redis Description: data will be stored in Redis's hashset with hash_name = `solutions`, key = `solutoin_id`, value = `solution_content`.\n",
    "\n",
    "##### error_keyword_dataset.json \n",
    "* Dataset Description: error_keyword_dataset.json stores different error and exceptoin logs' keywrods that are generated from different applications' product environment. \n",
    "* Redis: error_keyword_dataset.json will be stored in Redis's hashset with hash_name=`error_keyword`, key = `error_keyword_id`, value = `error_keyword_content`.\n",
    "\n",
    "##### solution_error_keyword_mapping.json\n",
    "* Dataset Description: solution_error_keyword_mapping.json stores `solution` and `error_keyword` mapping relationships. In ER-Diagram Design they are Multiple to Multiple. \n",
    "* Redis: solution_error_keyword_mapping.json will be stored in Redis's hashset with hash_name=`solution_error_keyword_mapping`, key = `error_keyword_id`, value = `{s_id_1, s_id_2, s_id_3, ...}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd8367b8-bd0f-4450-88e3-6c590a5f599f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write_error_solution_mapping_to_redis\n",
      "traverse redis hash table with name solution_error_keyword_mapping\n",
      "traverse_hash_table key: b'1', value: b'{\"py/set\": [\"1\", \"2\"]}'\n",
      "traverse_hash_table key: b'2', value: b'{\"py/set\": [\"1\", \"2\"]}'\n",
      "traverse_hash_table key: b'3', value: b'{\"py/set\": [\"1\", \"2\"]}'\n",
      "traverse_hash_table key: b'4', value: b'{\"py/set\": [\"1\", \"2\"]}'\n",
      "traverse_hash_table key: b'5', value: b'{\"py/set\": [\"1\", \"2\"]}'\n",
      "traverse_hash_table key: b'6', value: b'{\"py/set\": [\"1\", \"2\"]}'\n",
      "traverse_hash_table key: b'7', value: b'{\"py/set\": [\"1\", \"2\"]}'\n",
      "traverse_hash_table key: b'8', value: b'{\"py/set\": [\"1\", \"2\"]}'\n",
      "traverse_hash_table key: b'9', value: b'{\"py/set\": [\"1\", \"2\"]}'\n",
      "traverse_hash_table key: b'10', value: b'{\"py/set\": [\"3\"]}'\n",
      "traverse_hash_table key: b'11', value: b'{\"py/set\": [\"3\"]}'\n",
      "traverse_hash_table key: b'12', value: b'{\"py/set\": [\"3\"]}'\n",
      "traverse_hash_table key: b'13', value: b'{\"py/set\": [\"3\"]}'\n",
      "traverse_hash_table key: b'14', value: b'{\"py/set\": [\"3\"]}'\n",
      "traverse_hash_table key: b'15', value: b'{\"py/set\": [\"4\"]}'\n",
      "traverse_hash_table key: b'16', value: b'{\"py/set\": [\"5\"]}'\n",
      "traverse_hash_table key: b'17', value: b'{\"py/set\": [\"6\"]}'\n",
      "traverse_hash_table key: b'18', value: b'{\"py/set\": [\"6\"]}'\n",
      "traverse_hash_table key: b'19', value: b'{\"py/set\": [\"7\"]}'\n",
      "traverse_hash_table key: b'20', value: b'{\"py/set\": [\"7\"]}'\n",
      "traverse_hash_table key: b'21', value: b'{\"py/set\": [\"7\"]}'\n",
      "traverse_hash_table key: b'22', value: b'{\"py/set\": [\"7\"]}'\n",
      "traverse_hash_table key: b'23', value: b'{\"py/set\": [\"7\"]}'\n",
      "traverse_hash_table key: b'24', value: b'{\"py/set\": [\"7\"]}'\n",
      "traverse_hash_table key: b'25', value: b'{\"py/set\": [\"7\"]}'\n",
      "traverse_hash_table key: b'30', value: b'{\"py/set\": [\"8\"]}'\n",
      "traverse_hash_table key: b'31', value: b'{\"py/set\": [\"8\"]}'\n",
      "traverse_hash_table key: b'32', value: b'{\"py/set\": [\"8\"]}'\n",
      "clean redis hash table with name solution_error_keyword_mapping\n",
      "traverse redis hash table with name solution_error_keyword_mapping\n",
      "traverse_hash_table with hash_name solution_error_keyword_mapping got empty key set, return!\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import redis \n",
    "import jsonpickle\n",
    "\n",
    "def get_redis_client(host, port, password):\n",
    "    redis_client = redis.Redis(host = host, port = port, db = 0, password = password, ssl=True)\n",
    "    if redis_client is not None: \n",
    "        print(\"Success retrieve redis client\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve redis client\")\n",
    "\n",
    "def load_solution_dataset(solution_dataset_path):\n",
    "    json_data = None \n",
    "    with open(solution_dataset_path, 'r') as dataset_file:\n",
    "        json_data = json.load(dataset_file)\n",
    "    \n",
    "    print(f\"#load_solution_dataset ret json_data non-none status: {json_data is not None}\")\n",
    "    return json_data\n",
    "\n",
    "def clean_hash_table(redis_client, hash_name):\n",
    "    if redis_client is None:\n",
    "        print(f\"#clean_hash_table with hash_name {hash_name} recv redis handler is None return!\")\n",
    "        return \n",
    "    redis_client.delete(hash_name)\n",
    "\n",
    "def traverse_hash_table(redis_client, hash_name):\n",
    "    if redis_client is None:\n",
    "        print(f\"traverse_hash_table with hash_name {hash_name} recv redis handler is None return!\")\n",
    "        return \n",
    "    k_set = redis_client.hkeys(hash_name)\n",
    "    if len(k_set) == 0:\n",
    "        print(f\"traverse_hash_table with hash_name {hash_name} got empty key set, return!\")\n",
    "        return \n",
    "    for key in k_set:\n",
    "        value = redis_client.hget(hash_name, key)\n",
    "        print(f\"traverse_hash_table key: {key}, value: {value}\")\n",
    "\n",
    "\n",
    "def insert_data_to_redis(redis_client, json_data_arr, hash_name):\n",
    "    print(f\"#insert_data_to_redis recv redis_client non-none status: {redis_client is not None}\\n json_data_arr non-none status {json_data_arr is not None}\\n, hash_name non-none status: {hash_name is not None}\")\n",
    "    \n",
    "    if redis_client is None:\n",
    "        print(\"#insert_data_to_redis recv redis client is None, return!\")\n",
    "        return \n",
    "    \n",
    "    if json_data_arr is None:\n",
    "        print(\"#insert_data_to_redis recv json_data_arr is None, return!\")\n",
    "        return \n",
    "    \n",
    "    if hash_name is None:\n",
    "        print(\"#insert_data_to_redis recv hash_name is None, return!\")\n",
    "        return \n",
    "    \n",
    "    w_cnt = 0\n",
    "    r_cnt = 0\n",
    "    for item in json_data_arr:\n",
    "        index = str(item[\"id\"])\n",
    "        json_data = json.dumps(item)\n",
    "        w_cnt += 1\n",
    "        print(f\"write to redis with index {index}, with value len {len(json_data)}\")\n",
    "        redis_client.hset(hash_name, index, json_data)\n",
    "        value = redis_client.hget(hash_name, index)\n",
    "        if value is not None:\n",
    "            r_cnt += 1\n",
    "        #print(f\"#insert_data_to_redis retrieve with hash_name: {hash_name}, index: {index}, value={value}\")    \n",
    "    return (r_cnt, w_cnt)\n",
    "  \n",
    "def insert_keyword_solutions_mapping(redis_client, json_data_arr, hash_name):\n",
    "    print(f\"insert_keyword_solutions_mapping recv redis_client non-none status: {redis_client is not None}\\njson_data_arr non-none status: {json_data_arr is not None}\\nhash_name non-none status {hash_name is not None}\")\n",
    "\n",
    "    # hash_name = 'keyword_solutions_mapping'\n",
    "    # key: keyword_id \n",
    "    # value: set(solution_id)\n",
    "    for item in json_data_arr: \n",
    "        # step-1: find by given hash_name && keyword_id to get the hash set or create a new has set \n",
    "        # step-2: append current solution_id to the find/new created hash set \n",
    "        # step-3: save the key & value pair to redis (key: keyword_id, value: hash set(solution_id_1, solution_id_2,...))\n",
    "\n",
    "        # s_id: solution id, k_id: error_keyword id\n",
    "        # store key: k_id, value: {s_id_1, s_id_2,}\n",
    "        if item is None or item['s_id'] is None or item['k_id'] is None:\n",
    "            print(f\"insert_keyword_solutions_mapping recv invalid item ignore and continue\")\n",
    "            continue \n",
    "        s_id = item['s_id']\n",
    "        k_id = item['k_id']\n",
    "        ret_set_str = redis_client.hget(hash_name, k_id)\n",
    "        ret_set = None\n",
    "        if ret_set_str is None:\n",
    "            ret_set = set()\n",
    "        else:\n",
    "            print(f\"hash set is not none {ret_set_str}\")\n",
    "            ret_set = jsonpickle.decode(ret_set_str)\n",
    "            print(f\"hash set str is converted into set {type(ret_set)}\")\n",
    "        \n",
    "        ret_set.add(s_id)\n",
    "        set_json_str = jsonpickle.encode(ret_set)\n",
    "        print(f\"#insert_keyword_solutions_mapping write key: {k_id}, value: {set_json_str} to redis\")\n",
    "        redis_client.hset(hash_name, k_id, set_json_str)\n",
    "    print(f\"insert_keyword_solutions_mapping finish && return\")\n",
    "    return \n",
    "\n",
    "def insert_keyword_modules_mapping(redis_client, json_data_arr, hash_name):\n",
    "    print(f\"insert_keyword_modules_mapping recv redis_client non-none status: {redis_client is not None}\\njson_data_arr non-none status: {json_data_arr is not None}\\nhash_name non-none status {hash_name is not None}\")\n",
    "\n",
    "    # hash_name = 'keyword_modules_mapping'\n",
    "    # key: keyword_id\n",
    "    # value: set(module_id)\n",
    "    for item in json_data_arr:\n",
    "        # step-1: find by given hash_name && keyword_id to get the hash set or create a new hash set  \n",
    "        # {\"m_id\": \"xxx\", \"k_id\": \"xxx\"}\n",
    "        m_id = item['m_id']\n",
    "        k_id = item['k_id']\n",
    "        print (f\"#insert_keyword_modules_mapping recv m_id:{m_id}, k_id: {k_id}\\n\")\n",
    "        if m_id is not None and k_id is not None:\n",
    "            print(f\"#insert\")\n",
    "        else:\n",
    "            print(f\"#insert_keyword_modules_mapping\")\n",
    "        #>>  redis_client.hget(hash_name, )\n",
    "        # step-2: append current module_id to the find/new create hahs set\n",
    "        # step-3: save the key & value pair to tredis (key: keyword_id, value: hash set(module_id_1, module_id_2, ...))\n",
    "        print (item)\n",
    "\n",
    "    print(\"insert_keyword_modules_mapping finish && return\")\n",
    "    return\n",
    "\n",
    "def write_solution_dataset_to_redis():\n",
    "    print(\"BEGIN write_solution_dataset_to_redis \")\n",
    "    # redis hash table specific hash_name \n",
    "    hash_name = 'solutions'\n",
    "    \n",
    "    # dataset local directory  \n",
    "    local_dataset_path ='/Workspace/idm_chatbot/dataset/solutions_dataset.json'\n",
    "\n",
    "    # here we convert json data content into json array \n",
    "    json_data_arr = load_solution_dataset(local_dataset_path)\n",
    "    \n",
    "    # redis basic configuration \n",
    "    redis_host = \"\"\n",
    "    redis_port = 6380\n",
    "    redis_password = \"=\"\n",
    "    \n",
    "    # create redis client by given redis configuration items \n",
    "    redis_client = redis.Redis(host = redis_host, port = redis_port, db = 0, password = redis_password, ssl=True)\n",
    "    print(f\"redis_client non-null status {redis_client is not None}\")\n",
    "\n",
    "    # write json item to correspoinding hash table with specific hash name \n",
    "    (r_cnt, w_cnt) = insert_data_to_redis(redis_client, json_data_arr, hash_name=hash_name)\n",
    "    \n",
    "    # print final data results \n",
    "    print(f\"we total write to redis with hash value [{hash_name}] total write count: {w_cnt}, success read count {r_cnt}\")\n",
    "\n",
    "    # do not forget close redis client connection \n",
    "    redis_client.close()\n",
    "    print(\"END write_solution_dataset_to_redis \")\n",
    "\n",
    "\n",
    "def write_error_keyword_dataset_to_redis():\n",
    "    print(\"BEGIN write_error_keyword_dataset_to_redis \")\n",
    "    hash_name = 'error_keyword'\n",
    "    local_dataset_path = '/Workspace/idm_chatbot/dataset/error_keyword_dataset.json'\n",
    "\n",
    "    # here we convert json data content into json array \n",
    "    json_data_arr = load_solution_dataset(local_dataset_path)\n",
    "\n",
    "    # redis basic configuration \n",
    "    redis_host = \"\"\n",
    "    redis_port = 6380\n",
    "    redis_password = \"\"\n",
    "\n",
    "    # create redis client by given redis configuration items \n",
    "    redis_client = redis.Redis(host = redis_host, port = redis_port, db = 0, password = redis_password, ssl=True)\n",
    "    print(f\"redis_client non-null status {redis_client is not None}\")\n",
    "\n",
    "    # write json items to corresponding hash table with speicfic hash name \n",
    "    (r_cnt, w_cnt) = insert_data_to_redis(redis_client, json_data_arr, hash_name=hash_name)\n",
    "\n",
    "    # do not forget close redis client connection \n",
    "    redis_client.close()\n",
    "    print(\"END write_error_keyword_dataset_to_redis \")\n",
    "\n",
    "def write_error_solution_mapping_to_redis():\n",
    "    print(\"BEGIN write_solution_dataset_to_redis \")\n",
    "    hash_name = \"solution_error_keyword_mapping\"\n",
    "    local_dataset_path = '/Workspace/idm_chatbot/dataset/solution_error_keyword_mapping.json'\n",
    "\n",
    "    # here we convert json data content into json array \n",
    "    json_data_arr = load_solution_dataset(local_dataset_path)\n",
    "\n",
    "    # redis basic configuration \n",
    "    redis_host = \"hostname\"\n",
    "    redis_port = 6380\n",
    "    redis_password = \"password\"\n",
    "\n",
    "    # create redis client by given redis configuration items \n",
    "    redis_client = redis.Redis(host = redis_host, port = redis_port, db = 0, password = redis_password, ssl=True)\n",
    "    print(f\"redis_client non-null status {redis_client is not None}\")\n",
    "\n",
    "    # traverse each item in json arrary and insert each item to redis by given specific hash_name \n",
    "    insert_keyword_solutions_mapping(redis_client, json_data_arr, hash_name=hash_name)\n",
    "\n",
    "    # do not forget close redis client connection \n",
    "    redis_client.close()\n",
    "    print(\"END write_solution_dataset_to_redis \")\n",
    "\n",
    "#print(\"write_solution_dataset_to_redis\")\n",
    "#write_solution_dataset_to_redis()\n",
    "\n",
    "#print(\"write_error_keyword_dataset_to_redis write data\")\n",
    "#write_error_keyword_dataset_to_redis()\n",
    "\n",
    "print(\"write_error_solution_mapping_to_redis\")\n",
    "#write_error_solution_mapping_to_redis()\n",
    "\n",
    "hash_table_name = 'solution_error_keyword_mapping'\n",
    "redis_host = \"\"\n",
    "redis_port = 6380\n",
    "redis_password = \"=\"\n",
    "redis_client = redis.Redis(host = redis_host, port = redis_port, db = 0, password = redis_password, ssl=True)\n",
    "\n",
    "print(f\"traverse redis hash table with name {hash_table_name}\")\n",
    "traverse_hash_table(redis_client, hash_table_name)\n",
    "print(f\"clean redis hash table with name {hash_table_name}\")\n",
    "clean_hash_table(redis_client, hash_table_name)\n",
    "\n",
    "print(f\"traverse redis hash table with name {hash_table_name}\")\n",
    "traverse_hash_table(redis_client, hash_table_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65d9da0a-dab7-4a03-9013-2145a0afde45",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3-3 Read Dataset from Redis to DataFrame\n",
    "\n",
    "%md \n",
    "#### step 3-2 Execute Python Scripts to Load Error Diagnoise Related Datasets to Redis Hash Set \n",
    "\n",
    "##### solutions_dataset.json \n",
    "* Dataset Description: solutions_dataset.json stores different errors' solutoin steps and causes\n",
    "* Redis Description: data will be stored in Redis's hashset with hash_name = `solutions`, key = `solutoin_id`, value = `solution_content`.\n",
    "\n",
    "##### error_keyword_dataset.json \n",
    "* Dataset Description: error_keyword_dataset.json stores different error and exceptoin logs' keywrods that are generated from different applications' product environment. \n",
    "* Redis: error_keyword_dataset.json will be stored in Redis's hashset with hash_name=`error_keyword`, key = `error_keyword_id`, value = `error_keyword_content`.\n",
    "\n",
    "##### solution_error_keyword_mapping.json\n",
    "* Dataset Description: solution_error_keyword_mapping.json stores `solution` and `error_keyword` mapping relationships. In ER-Diagram Design they are Multiple to Multiple. \n",
    "* Redis: solution_error_keyword_mapping.json will be stored in Redis's hashset with hash_name=`solution_error_keyword_mapping`, key = `error_keyword_id`, value = `{s_id_1, s_id_2, s_id_3, ...}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c45e9f06-3e9c-49af-88d5-2c4eef82de1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# solution_dataset.json -> redis[solutions] -> dataframe[solutions]\n",
    "\n",
    "# error_keyword_dataset.json -> redis[error_keywrod] -> dataframe[error_keywords]\n",
    "\n",
    "# solution_error_keyword_mapping.json -> redis[solution_error_keyword_mapping] -> dataframe[solution_error_keyword_mapping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c21d6883-cb08-4da1-a889-7504c2a199ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### step 3-4 Execute Python Scripts to Load Module Diagnoise Related Datasets to Redis Hash Set \n",
    "\n",
    "##### modules_dataset.json \n",
    "* Dataset Description: solutions_dataset.json stores different errors' solutoin steps and causes\n",
    "* Redis: data will be stored in Redis's hashset with hash_name = `solutions`, key = `solutoin_id`, value = `solution_content`\n",
    "\n",
    "##### error_keyword_dataset.json \n",
    "* Dataset Description: error_keyword_dataset.json stores different error and exceptoin logs' keywrods that are generated from different applications' product environment. \n",
    "* Redis: error_keyword_dataset.json will be stored in Redis's hashset with hash_name=`error_keyword`, key = `error_keyword_id`, value = `error_keyword_content`.\n",
    "\n",
    "##### module_error_keyword_mapping.json\n",
    "* Dataset Description: module_error_keyword_mapping.json stores `module` and `error_keywrod` mapping relationships. In ER-Diagram Design they are Multiple to Multiple.\n",
    "* Redis module_error_keyword_mapping.json will be stored in Redis's hashset with hash_name=`module_error_keyword_mapping`, key=`error_keyword_id`, value=`{m_id_1, m_id_2, m_id_3, ...}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03cec4f7-14f7-42ae-b94e-afda3293ae6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create redis client \n",
      "Success retrieve redis client\n",
      "begin write data value to redis\n",
      "ret_value b'value'\n"
     ]
    }
   ],
   "source": [
    "# here we create connection to our Azure Redis && try to insert data to redis \n",
    "import json \n",
    "import redis \n",
    "\n",
    "def write_json_to_redis(redis_client, hash_name, field_name, json_data):\n",
    "    if redis_client is None:\n",
    "        print(\"redis client is None return!\")\n",
    "        return \n",
    "    print(f\"begin write data {json_data} to redis\")\n",
    "    redis_client.hset(hash_name, field_name, json_data)\n",
    "    ret_value = redis_client.hget(hash_name, field_name)\n",
    "    print(f\"ret_value {ret_value}\")\n",
    "\n",
    "\n",
    "print(\"create redis client \")\n",
    "redis_host = \"hostname\"\n",
    "redis_port = 6380\n",
    "redis_password = \"=\"\n",
    "redis_client = redis.Redis(host=redis_host, port=redis_port, db=0, password=redis_password, ssl=True)\n",
    "\n",
    "if redis_client is None: \n",
    "    print(\"Fail to retrieve redis client\")\n",
    "else:\n",
    "    print(\"Success retrieve redis client\")\n",
    "\n",
    "hash_value='hash_value'\n",
    "hash_field='hash_field'\n",
    "value = 'value'\n",
    "(w_cnt, r_cnt) = write_json_to_redis(redis_client, hash_value, hash_field, value)\n",
    "\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1080132038651403,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "0.prepare_chatbot_training_dataset",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
